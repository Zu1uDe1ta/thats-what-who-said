{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `02_transformer_model.ipynb`\n",
    "\n",
    "*Attribution note: portions of code in this notebook are borrowed from [another notebook](https://github.com/disinfo-detectors/tweet-turing-test/blob/main/src/05_BERT_fine_tuner.ipynb), which was a notebook written by one of our team members (Justin Minnion) for another class (DSCI 591/592).*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 - Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.1 - Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from python standard library\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# huggingface packages\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.2 - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file locations\n",
    "DATA_DIR = Path(\"./data\")\n",
    "DATA_DIR_PROCESSED = DATA_DIR / \"processed\"\n",
    "PROCESSED_DATA = DATA_DIR_PROCESSED / \"script_data_processed.csv\"\n",
    "\n",
    "MODEL_DIR = DATA_DIR / \"models\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.3 - Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_df = pd.read_csv(\n",
    "    filepath_or_buffer=PROCESSED_DATA,\n",
    "    header=0,\n",
    "    index_col=0,\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>scene</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>directed_by</th>\n",
       "      <th>written_by</th>\n",
       "      <th>writer1</th>\n",
       "      <th>writer2</th>\n",
       "      <th>writer3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good. How are things at the library?</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais &amp; Stephen Merchant and Greg Daniels</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "      <td>Stephen Merchant</td>\n",
       "      <td>Greg Daniels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais &amp; Stephen Merchant and Greg Daniels</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "      <td>Stephen Merchant</td>\n",
       "      <td>Greg Daniels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>michael</td>\n",
       "      <td>So you've come to the master for guidance? Is this what you're saying, grasshopper?</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais &amp; Stephen Merchant and Greg Daniels</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "      <td>Stephen Merchant</td>\n",
       "      <td>Greg Daniels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  episode  title  ...        writer1           writer2       writer3\n",
       "0       1        1  Pilot  ...  Ricky Gervais  Stephen Merchant  Greg Daniels\n",
       "1       1        1  Pilot  ...  Ricky Gervais  Stephen Merchant  Greg Daniels\n",
       "2       1        1  Pilot  ...  Ricky Gervais  Stephen Merchant  Greg Daniels\n",
       "\n",
       "[3 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54267.000000</td>\n",
       "      <td>54267.000000</td>\n",
       "      <td>54267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.538099</td>\n",
       "      <td>12.490003</td>\n",
       "      <td>4190.521606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.349106</td>\n",
       "      <td>7.286262</td>\n",
       "      <td>2294.821819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>6153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>8157.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             season       episode         scene\n",
       "count  54267.000000  54267.000000  54267.000000\n",
       "mean       5.538099     12.490003   4190.521606\n",
       "std        2.349106      7.286262   2294.821819\n",
       "min        1.000000      1.000000      1.000000\n",
       "25%        3.000000      6.000000   2325.000000\n",
       "50%        6.000000     12.000000   4215.000000\n",
       "75%        8.000000     18.000000   6153.000000\n",
       "max        9.000000     28.000000   8157.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine numeric fields\n",
    "script_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54267 entries, 0 to 54266\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   season       54267 non-null  int64 \n",
      " 1   episode      54267 non-null  int64 \n",
      " 2   title        54267 non-null  object\n",
      " 3   scene        54267 non-null  int64 \n",
      " 4   speaker      54267 non-null  object\n",
      " 5   line         54267 non-null  object\n",
      " 6   directed_by  54267 non-null  object\n",
      " 7   written_by   54267 non-null  object\n",
      " 8   writer1      54267 non-null  object\n",
      " 9   writer2      9816 non-null   object\n",
      " 10  writer3      699 non-null    object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 29.1 MB\n"
     ]
    }
   ],
   "source": [
    "script_df.info(memory_usage='deep')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dataset isn't particularly large, we can improve performance / memory footprint if we are more prescriptive with `dtype` settings. At a minimum we should aim for no \"`object`\" type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54267 entries, 0 to 54266\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   season       54267 non-null  int8    \n",
      " 1   episode      54267 non-null  int8    \n",
      " 2   title        54267 non-null  string  \n",
      " 3   scene        54267 non-null  int16   \n",
      " 4   speaker      54267 non-null  string  \n",
      " 5   line         54267 non-null  string  \n",
      " 6   directed_by  54267 non-null  category\n",
      " 7   written_by   54267 non-null  string  \n",
      " 8   writer1      54267 non-null  category\n",
      " 9   writer2      9816 non-null   category\n",
      " 10  writer3      699 non-null    category\n",
      "dtypes: category(4), int16(1), int8(2), string(4)\n",
      "memory usage: 17.4 MB\n"
     ]
    }
   ],
   "source": [
    "dtype_mapping = {\n",
    "    'season': 'int8',\n",
    "    'episode': 'int8',\n",
    "    'title': 'string',\n",
    "    'scene': 'int16',\n",
    "    'speaker': 'string',    # could be category if we limit to top 10 speakers\n",
    "    'line': 'string',\n",
    "    'directed_by': 'category',\n",
    "    'written_by': 'string',\n",
    "    'writer1': 'category',\n",
    "    'writer2': 'category',\n",
    "    'writer3': 'category',\n",
    "}\n",
    "\n",
    "script_df = script_df.astype(dtype_mapping)\n",
    "\n",
    "script_df.info(memory_usage='deep')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Basic Transformer\n",
    "\n",
    "Attempting a basic transformer model without too much customization to establish a baseline (within transformer-type models) for performance.\n",
    "\n",
    "**Task**: Sequence Classification (Binary)\n",
    "\n",
    "**Classes**: \n",
    " - Positive (1): \"Dwight\" - a line is spoken by the character Dwight K. Schrute (played by Rainn Wilson).\n",
    " - Negative (0): \"Not Dwight\" - a line is spoken by any other character than Dwight.\n",
    "\n",
    "**Data**:\n",
    " - `speaker` as pre-cursor to class label. Limited to top-10 most frequent speakers based on number of lines in dataset\n",
    " - `line` as sequence text.\n",
    "\n",
    "**Encoding**:\n",
    " - Tokenizer: DistilBertTokenizerFast\n",
    " - Max Sequence Length: 128\n",
    " - Padding: True\n",
    " - Truncate: True\n",
    "\n",
    "**Training**:\n",
    " - Train/Test/Validation Split: 50/25/25\n",
    "\n",
    "**Notes**:\n",
    " - Class imbalance is present (positive: 6,752; negative: 32,668; about `1:4.8` imbalance ratio).\n",
    " - Vocabulary: no modifications made to pretrained transformer's vocabulary.\n",
    " - Secondary data: no inclusion of secondary data (director/writer credits)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Dataset - Convert `pandas` -> 🤗 `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good. How are things at the library?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michael</td>\n",
       "      <td>So you've come to the master for guidance? Is this what you're saying, grasshopper?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>michael</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54257</th>\n",
       "      <td>kevin</td>\n",
       "      <td>No, but maybe the reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>oscar</td>\n",
       "      <td>You're not gay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>erin</td>\n",
       "      <td>How did you do it? How did you capture what it was really like? How we felt and how made each other laugh and how we got through the day? How did you do it? Also, how do cameras work?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54265</th>\n",
       "      <td>jim</td>\n",
       "      <td>I sold paper at this company for 12 years. My job was to speak to clients on the phone about quantities and types of copier paper. Even if I didn't love every minute of it, everything I have, I owe to this job. This stupid...wonderful...boring...amazing job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54266</th>\n",
       "      <td>pam</td>\n",
       "      <td>I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker                                                                                                                                                                                                                                                                line\n",
       "0      michael                                                                                                                                                                                      All right Jim. Your quarterlies look very good. How are things at the library?\n",
       "1          jim                                                                                                                                                                                                                          Oh, I told you. I couldn't close it. So...\n",
       "2      michael                                                                                                                                                                                 So you've come to the master for guidance? Is this what you're saying, grasshopper?\n",
       "3          jim                                                                                                                                                                                                                          Actually, you called me in here, but yeah.\n",
       "4      michael                                                                                                                                                                                                                     All right. Well, let me show you how it's done.\n",
       "...        ...                                                                                                                                                                                                                                                                 ...\n",
       "54257    kevin                                                                                                                                                                                                                                         No, but maybe the reason...\n",
       "54258    oscar                                                                                                                                                                                                                                                     You're not gay.\n",
       "54260     erin                                                                             How did you do it? How did you capture what it was really like? How we felt and how made each other laugh and how we got through the day? How did you do it? Also, how do cameras work?\n",
       "54265      jim  I sold paper at this company for 12 years. My job was to speak to clients on the phone about quantities and types of copier paper. Even if I didn't love every minute of it, everything I have, I owe to this job. This stupid...wonderful...boring...amazing job.\n",
       "54266      pam            I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?\n",
       "\n",
       "[39420 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit to top 10 most frequent speakers\n",
    "top_10_speaker_list = script_df['speaker'].value_counts(normalize=True).nlargest(10).index.tolist()\n",
    "columns_to_keep = ['speaker', 'line']\n",
    "\n",
    "script_df_subset = script_df.loc[script_df['speaker'].isin(top_10_speaker_list), columns_to_keep]\n",
    "\n",
    "script_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the 'line' column to be 'text'\n",
    "script_df_subset = script_df_subset.rename(columns={'line': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32668\n",
       "1     6752\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class label column\n",
    "dwight_mask = (script_df_subset['speaker'] == 'dwight')\n",
    "\n",
    "# new column of zeros\n",
    "script_df_subset['label'] = 0\n",
    "\n",
    "# apply the Dwight mask (as seen in the CPR scene of S05E14 \"Stress Relief\")\n",
    "script_df_subset.loc[dwight_mask, 'label'] = 1\n",
    "\n",
    "# adjust dtype\n",
    "script_df_subset['label'] = script_df_subset['label'].astype('int8')    \n",
    "    # would love to use 'category', but not implemented in 🤗 datasets\n",
    "\n",
    "# check results\n",
    "script_df_subset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39420 entries, 0 to 54266\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   speaker  39420 non-null  string\n",
      " 1   text     39420 non-null  string\n",
      " 2   label    39420 non-null  int8  \n",
      "dtypes: int8(1), string(2)\n",
      "memory usage: 7.0 MB\n"
     ]
    }
   ],
   "source": [
    "script_df_subset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92d868c8b3b43adb909546d68bf0589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/39420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finally, convert to 🤗 dataset object\n",
    "#   drop 'speaker' by way of not including it\n",
    "dataset_full: Dataset = Dataset.from_pandas(script_df_subset[['text', 'label']].reset_index(drop=False)) \\\n",
    "                    .cast_column('label', ClassLabel(names=['not_dwight', 'dwight']))\n",
    "\n",
    "# make sure we got the class labels mapped correctly\n",
    "assert (dataset_full.features['label'].str2int('dwight') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'label'],\n",
       "    num_rows: 39420\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Train/Test/Val Split\n",
    "\n",
    "As of v2.12.0, the 🤗 Datasets implementation of `train_test_split` is limited to outputting **two** splits only (train/test), so we'll perform the split twice to obtain train, test, and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 19710\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 9855\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 9855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters\n",
    "train_size = 0.50\n",
    "test_size = 0.25\n",
    "valid_size = 0.25\n",
    "\n",
    "assert sum([train_size, test_size, valid_size]) == 1.0\n",
    "\n",
    "split_random_seed = 27  # for Weird Al fans\n",
    "\n",
    "first_split = dataset_full.train_test_split(\n",
    "    test_size=(1.0 - train_size),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "second_split = first_split['test'].train_test_split(\n",
    "    test_size=((valid_size) / (test_size + valid_size)),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "ds_dict = DatasetDict({\n",
    "    'train': first_split['train'],\n",
    "    'test': second_split['train'],\n",
    "    'valid': second_split['test']\n",
    "})\n",
    "\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio positive/negative is:\t1 to 4.8\n"
     ]
    }
   ],
   "source": [
    "# confirm stratified sample\n",
    "num_negative = ds_dict['train'].to_pandas()['label'].value_counts()[0]\n",
    "num_positive = ds_dict['train'].to_pandas()['label'].value_counts()[1]\n",
    "\n",
    "print(f\"ratio positive/negative is:\\t1 to {num_negative/num_positive:0.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tokenize and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6ef9f614b04e468704e77b4abcb174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jminn\\.envs\\ds_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jminn\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db83623b0aae464980087eb90d41281a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7597847dc069466399df0c41bc16113d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8e81b996e5446f89b0f8039fe42f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c664337748b44c6ae9ef69de3a64fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f31dd8417d47298777cec07731e2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f47129a9164619bc11e64efef34154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], \n",
    "                     padding='longest', \n",
    "                     truncation=True, \n",
    "                     return_tensors='pt',\n",
    "                     max_length=128)\n",
    "\n",
    "ds_tokenized = ds_dict.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tokens(tokenizer, encoded_text: dict):\n",
    "    '''Prints the provided encoded text as its original text and as its tokenized form.\n",
    "        - tokenizer is an instantiated huggingface tokenizer (sub-subclass of PreTrainedTokenizerBase)\n",
    "        - encoded_text is the dict created from one element of a huggingface dataset\n",
    "        '''\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    inverse_vocab = {v: k for (k, v) in vocab.items()}\n",
    "\n",
    "    tokens_list = [inverse_vocab[i] for i in encoded_text['input_ids']]\n",
    "    tokens_list_attention = [tokens_list[i] for i in range(len(tokens_list)) if (encoded_text['attention_mask'][i] == 1)]\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Original text:\\n\\t{encoded_text['text']}\", end=\"\\n\\n\")\n",
    "    print(f\"Label:\\t{encoded_text['label']}\", end=\"\\n\\n\")\n",
    "    print(f\"Tokenized form:\\n\\t{' '.join(tokens_list)}\", end=\"\\n\\n\")\n",
    "    print(f\"Tokens as a list:\\n\\t{tokens_list}\", end=\"\\n\\n\")\n",
    "    print(f\"Tokens as a list, attention mask applied:\\n\\t{tokens_list_attention}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original text:\n",
      "\t Birthday time is over! Now go make up for all the work you missed when you were taking your nap.  Many happy returns. \n",
      "\n",
      "Label:\t1\n",
      "\n",
      "Tokenized form:\n",
      "\t[CLS] birthday time is over ! now go make up for all the work you missed when you were taking your nap . many happy returns . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Tokens as a list:\n",
      "\t['[CLS]', 'birthday', 'time', 'is', 'over', '!', 'now', 'go', 'make', 'up', 'for', 'all', 'the', 'work', 'you', 'missed', 'when', 'you', 'were', 'taking', 'your', 'nap', '.', 'many', 'happy', 'returns', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['[CLS]', 'birthday', 'time', 'is', 'over', '!', 'now', 'go', 'make', 'up', 'for', 'all', 'the', 'work', 'you', 'missed', 'when', 'you', 'were', 'taking', 'your', 'nap', '.', 'many', 'happy', 'returns', '.', '[SEP]']\n",
      "\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "\tBy 2:00, Dwight will chose himself to be assistant to his own assistant, me.\n",
      "\n",
      "Label:\t0\n",
      "\n",
      "Tokenized form:\n",
      "\t[CLS] by 2 : 00 , dwight will chose himself to be assistant to his own assistant , me . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Tokens as a list:\n",
      "\t['[CLS]', 'by', '2', ':', '00', ',', 'dwight', 'will', 'chose', 'himself', 'to', 'be', 'assistant', 'to', 'his', 'own', 'assistant', ',', 'me', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['[CLS]', 'by', '2', ':', '00', ',', 'dwight', 'will', 'chose', 'himself', 'to', 'be', 'assistant', 'to', 'his', 'own', 'assistant', ',', 'me', '.', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_tokens(tokenizer, ds_tokenized['train'][27])\n",
    "inspect_tokens(tokenizer, ds_tokenized['test'][42])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Model\n",
    "\n",
    "Create model from pre-trained 🤗 transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f553db8ee47c400393f4b1eb6508fca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp.now().strftime(r'%Y%m%d_%H%M%S')  # yyyymmdd_hhmmss\n",
    "run_name = f\"basic_distilbert_{start_time}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # model output\n",
    "    run_name=run_name,\n",
    "    output_dir=MODEL_DIR / run_name,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    # training hyperparams\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation during training\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    log_level='warning',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5eefca687d04fc6a201ff25d2aa8c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c36b7863864d4899dbd53de5d6f26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a5c79c98ea4b7bbe1751573340b428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1662d4c05fd404aa656ab0462b91c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup training / evaluation metric\n",
    "#   Docs: https://huggingface.co/docs/evaluate/package_reference/main_classes#evaluate.combine\n",
    "#   Each of these metrics corresponds to a script from huggingface, below are the links for each script.\n",
    "#       accuracy:       https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "#       f1:             https://huggingface.co/spaces/evaluate-metric/f1\n",
    "#       precision:      https://huggingface.co/spaces/evaluate-metric/precision\n",
    "#       recall:         https://huggingface.co/spaces/evaluate-metric/recall\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "metric = evaluate.combine(evaluations=metric_list)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, setup the 🤗 Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jminn\\.envs\\ds_env\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c51c0c886034d188a94bb920ab67059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4413, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dd8930fc0646b3af2c9d3c048f86a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42805027961730957, 'eval_accuracy': 0.8305428716387621, 'eval_f1': 0.0324449594438007, 'eval_precision': 0.7368421052631579, 'eval_recall': 0.016587677725118485, 'eval_runtime': 17.4104, 'eval_samples_per_second': 566.04, 'eval_steps_per_second': 17.691, 'epoch': 1.0}\n",
      "{'loss': 0.373, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5ca80fc4b1492090ff3075445b3f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.446800172328949, 'eval_accuracy': 0.8272957889396245, 'eval_f1': 0.3304484657749803, 'eval_precision': 0.4918032786885246, 'eval_recall': 0.24881516587677724, 'eval_runtime': 17.4796, 'eval_samples_per_second': 563.802, 'eval_steps_per_second': 17.621, 'epoch': 2.0}\n",
      "{'loss': 0.277, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0be71a4bbb47e2b39530c904f5e983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5202162861824036, 'eval_accuracy': 0.8321664129883308, 'eval_f1': 0.26292335115864524, 'eval_precision': 0.5305755395683454, 'eval_recall': 0.17476303317535544, 'eval_runtime': 17.5137, 'eval_samples_per_second': 562.703, 'eval_steps_per_second': 17.586, 'epoch': 3.0}\n",
      "{'loss': 0.2014, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac32a7f485eb4a7e9d7fec84b67eadbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6362824440002441, 'eval_accuracy': 0.8148148148148148, 'eval_f1': 0.33756805807622503, 'eval_precision': 0.43580131208997186, 'eval_recall': 0.2754739336492891, 'eval_runtime': 17.5254, 'eval_samples_per_second': 562.326, 'eval_steps_per_second': 17.574, 'epoch': 4.0}\n",
      "{'loss': 0.1495, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4521e31baae40ada40358be43924a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7720897197723389, 'eval_accuracy': 0.8077118214104515, 'eval_f1': 0.34041072050121823, 'eval_precision': 0.41265822784810124, 'eval_recall': 0.2896919431279621, 'eval_runtime': 17.5274, 'eval_samples_per_second': 562.261, 'eval_steps_per_second': 17.572, 'epoch': 5.0}\n",
      "{'train_runtime': 588.4075, 'train_samples_per_second': 167.486, 'train_steps_per_second': 5.234, 'train_loss': 0.2884544446870878, 'epoch': 5.0}\n",
      "\n",
      "Training duration: 0 days 00:09:48.664755\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized['train'],\n",
    "    eval_dataset=ds_tokenized['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(\"\\nTraining duration:\", str(time_training))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()    # saves to self.args.output_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa992a576c44043baa3a8e2214b7ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84f9199c3604137b99c5126c597b0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb96577042a94aff84f652d013ee1552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_metrics = {}\n",
    "final_metrics['train'] = trainer.evaluate(eval_dataset=ds_tokenized['train'], metric_key_prefix='final_train')\n",
    "final_metrics['test']= trainer.evaluate(eval_dataset=ds_tokenized['test'], metric_key_prefix='final_test')\n",
    "final_metrics['valid'] = trainer.evaluate(eval_dataset=ds_tokenized['valid'], metric_key_prefix='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----TRAIN---------------\n",
      "     0.112 - final_train_loss\n",
      "     0.962 - final_train_accuracy\n",
      "     0.877 - final_train_f1\n",
      "     0.972 - final_train_precision\n",
      "     0.799 - final_train_recall\n",
      "    34.720 - final_train_runtime\n",
      "   567.687 - final_train_samples_per_second\n",
      "    17.742 - final_train_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "------TEST---------------\n",
      "     0.772 - final_test_loss\n",
      "     0.808 - final_test_accuracy\n",
      "     0.340 - final_test_f1\n",
      "     0.413 - final_test_precision\n",
      "     0.290 - final_test_recall\n",
      "    17.456 - final_test_runtime\n",
      "   564.549 - final_test_samples_per_second\n",
      "    17.644 - final_test_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "-----VALID---------------\n",
      "     0.793 - validation_loss\n",
      "     0.803 - validation_accuracy\n",
      "     0.331 - validation_f1\n",
      "     0.396 - validation_precision\n",
      "     0.284 - validation_recall\n",
      "    17.408 - validation_runtime\n",
      "   566.124 - validation_samples_per_second\n",
      "    17.693 - validation_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for split in final_metrics:\n",
    "    print(f\"\\n{split.upper():->10}{'-'*15}\")\n",
    "    for k, v in final_metrics[split].items():\n",
    "        print(f\"{v:>10.3f} - {k}\")\n",
    "    print(\"-\"*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
