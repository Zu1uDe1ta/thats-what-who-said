{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `02_transformer_model.ipynb`\n",
    "\n",
    "*Attribution note: portions of code in this notebook are borrowed from [another notebook](https://github.com/disinfo-detectors/tweet-turing-test/blob/main/src/05_BERT_fine_tuner.ipynb), which was a notebook written by one of our team members (Justin Minnion) for another class (DSCI 591/592).*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 - Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.1 - Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from python standard library\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# huggingface packages\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.2 - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file locations\n",
    "DATA_DIR = Path(\"./data\")\n",
    "DATA_DIR_PROCESSED = DATA_DIR / \"processed\"\n",
    "PROCESSED_DATA = DATA_DIR_PROCESSED / \"script_data_processed.csv\"\n",
    "\n",
    "MODEL_DIR = DATA_DIR / \"models\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.3 - Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.4 - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tokens(tokenizer, encoded_text: dict):\n",
    "    '''Prints the provided encoded text as its original text and as its tokenized form.\n",
    "        - tokenizer is an instantiated huggingface tokenizer (sub-subclass of PreTrainedTokenizerBase)\n",
    "        - encoded_text is the dict created from one element of a huggingface dataset\n",
    "        '''\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    inverse_vocab = {v: k for (k, v) in vocab.items()}\n",
    "\n",
    "    tokens_list = [inverse_vocab[i] for i in encoded_text['input_ids']]\n",
    "    tokens_list_attention = [tokens_list[i] for i in range(len(tokens_list)) if (encoded_text['attention_mask'][i] == 1)]\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Original text:\\n\\t{encoded_text['text']}\", end=\"\\n\\n\")\n",
    "    print(f\"Label:\\t{encoded_text['label']}\", end=\"\\n\\n\")\n",
    "    print(f\"Tokenized form:\\n\\t{' '.join(tokens_list)}\", end=\"\\n\\n\")\n",
    "    print(f\"Tokens as a list:\\n\\t{tokens_list}\", end=\"\\n\\n\")\n",
    "    print(f\"Tokens as a list, attention mask applied:\\n\\t{tokens_list_attention}\", end=\"\\n\\n\")\n",
    "\n",
    "def informal_test(tokenizer, model, test_line):\n",
    "    with torch.no_grad():\n",
    "        beets_or_not = tokenizer(test_line, return_tensors='pt').to('cuda')\n",
    "        result = model(**beets_or_not)\n",
    "        print(f'{\"Test Line: \":>20}', f'\"{test_line}\"')\n",
    "        print(f'{\"Predicted Speaker: \":>20}', model.config.id2label[result.logits.argmax().item()])\n",
    "        print(f'{\"Logits: \":>20}', result.logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_df = pd.read_csv(\n",
    "    filepath_or_buffer=PROCESSED_DATA,\n",
    "    header=0,\n",
    "    index_col=0,\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>title</th>\n",
       "      <th>scene</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>directed_by</th>\n",
       "      <th>written_by</th>\n",
       "      <th>writer1</th>\n",
       "      <th>writer2</th>\n",
       "      <th>writer3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good. How are things at the library?</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais &amp; Stephen Merchant and Greg Daniels</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "      <td>Stephen Merchant</td>\n",
       "      <td>Greg Daniels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais &amp; Stephen Merchant and Greg Daniels</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "      <td>Stephen Merchant</td>\n",
       "      <td>Greg Daniels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>1</td>\n",
       "      <td>michael</td>\n",
       "      <td>So you've come to the master for guidance? Is this what you're saying, grasshopper?</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais &amp; Stephen Merchant and Greg Daniels</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "      <td>Stephen Merchant</td>\n",
       "      <td>Greg Daniels</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  episode  title  scene  speaker  \\\n",
       "0       1        1  Pilot      1  michael   \n",
       "1       1        1  Pilot      1      jim   \n",
       "2       1        1  Pilot      1  michael   \n",
       "\n",
       "                                                                                  line  \\\n",
       "0       All right Jim. Your quarterlies look very good. How are things at the library?   \n",
       "1                                           Oh, I told you. I couldn't close it. So...   \n",
       "2  So you've come to the master for guidance? Is this what you're saying, grasshopper?   \n",
       "\n",
       "  directed_by                                         written_by  \\\n",
       "0  Ken Kwapis  Ricky Gervais & Stephen Merchant and Greg Daniels   \n",
       "1  Ken Kwapis  Ricky Gervais & Stephen Merchant and Greg Daniels   \n",
       "2  Ken Kwapis  Ricky Gervais & Stephen Merchant and Greg Daniels   \n",
       "\n",
       "         writer1           writer2       writer3  \n",
       "0  Ricky Gervais  Stephen Merchant  Greg Daniels  \n",
       "1  Ricky Gervais  Stephen Merchant  Greg Daniels  \n",
       "2  Ricky Gervais  Stephen Merchant  Greg Daniels  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54267.000000</td>\n",
       "      <td>54267.000000</td>\n",
       "      <td>54267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.538099</td>\n",
       "      <td>12.490003</td>\n",
       "      <td>4190.521606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.349106</td>\n",
       "      <td>7.286262</td>\n",
       "      <td>2294.821819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>6153.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>8157.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             season       episode         scene\n",
       "count  54267.000000  54267.000000  54267.000000\n",
       "mean       5.538099     12.490003   4190.521606\n",
       "std        2.349106      7.286262   2294.821819\n",
       "min        1.000000      1.000000      1.000000\n",
       "25%        3.000000      6.000000   2325.000000\n",
       "50%        6.000000     12.000000   4215.000000\n",
       "75%        8.000000     18.000000   6153.000000\n",
       "max        9.000000     28.000000   8157.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine numeric fields\n",
    "script_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54267 entries, 0 to 54266\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   season       54267 non-null  int64 \n",
      " 1   episode      54267 non-null  int64 \n",
      " 2   title        54267 non-null  object\n",
      " 3   scene        54267 non-null  int64 \n",
      " 4   speaker      54267 non-null  object\n",
      " 5   line         54267 non-null  object\n",
      " 6   directed_by  54267 non-null  object\n",
      " 7   written_by   54267 non-null  object\n",
      " 8   writer1      54267 non-null  object\n",
      " 9   writer2      9816 non-null   object\n",
      " 10  writer3      699 non-null    object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 29.1 MB\n"
     ]
    }
   ],
   "source": [
    "script_df.info(memory_usage='deep')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dataset isn't particularly large, we can improve performance / memory footprint if we are more prescriptive with `dtype` settings. At a minimum we should aim for no \"`object`\" type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54267 entries, 0 to 54266\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   season       54267 non-null  int8    \n",
      " 1   episode      54267 non-null  int8    \n",
      " 2   title        54267 non-null  string  \n",
      " 3   scene        54267 non-null  int16   \n",
      " 4   speaker      54267 non-null  string  \n",
      " 5   line         54267 non-null  string  \n",
      " 6   directed_by  54267 non-null  category\n",
      " 7   written_by   54267 non-null  string  \n",
      " 8   writer1      54267 non-null  category\n",
      " 9   writer2      9816 non-null   category\n",
      " 10  writer3      699 non-null    category\n",
      "dtypes: category(4), int16(1), int8(2), string(4)\n",
      "memory usage: 17.4 MB\n"
     ]
    }
   ],
   "source": [
    "dtype_mapping = {\n",
    "    'season': 'int8',\n",
    "    'episode': 'int8',\n",
    "    'title': 'string',\n",
    "    'scene': 'int16',\n",
    "    'speaker': 'string',    # could be category if we limit to top 10 speakers\n",
    "    'line': 'string',\n",
    "    'directed_by': 'category',\n",
    "    'written_by': 'string',\n",
    "    'writer1': 'category',\n",
    "    'writer2': 'category',\n",
    "    'writer3': 'category',\n",
    "}\n",
    "\n",
    "script_df = script_df.astype(dtype_mapping)\n",
    "\n",
    "script_df.info(memory_usage='deep')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Basic Transformer\n",
    "\n",
    "Attempting a basic transformer model without too much customization to establish a baseline (within transformer-type models) for performance.\n",
    "\n",
    "**Task**: Sequence Classification (Binary)\n",
    "\n",
    "**Classes**: \n",
    " - Positive (1): \"Dwight\" - a line is spoken by the character Dwight K. Schrute (played by Rainn Wilson).\n",
    " - Negative (0): \"Not Dwight\" - a line is spoken by any other character than Dwight.\n",
    "\n",
    "**Data**:\n",
    " - `speaker` as pre-cursor to class label. Limited to top-10 most frequent speakers based on number of lines in dataset\n",
    " - `line` as sequence text.\n",
    "\n",
    "**Encoding**:\n",
    " - Tokenizer: DistilBertTokenizerFast\n",
    " - Max Sequence Length: 128\n",
    " - Padding: True\n",
    " - Truncate: True\n",
    "\n",
    "**Pretrained Model**:\n",
    " - DistilBert (`distilbert-base-uncased`) [(link: huggingface.co)](https://huggingface.co/distilbert-base-uncased) - Intended to mimic the standard \"BERTbase\" model but in a smaller/faster/more efficient way.\n",
    " - Citation: Sanh et al. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" (2019) - [https://arxiv.org/pdf/1910.01108.pdf](https://arxiv.org/pdf/1910.01108.pdf)\n",
    "\n",
    "**Training**:\n",
    " - Train/Test/Validation Split: 50/25/25\n",
    "\n",
    "**Notes**:\n",
    " - Class imbalance is present (positive: 6,752; negative: 32,668; about `1:4.8` imbalance ratio).\n",
    " - Vocabulary: no modifications made to pretrained transformer's vocabulary.\n",
    " - Secondary data: no inclusion of secondary data (director/writer credits)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Dataset - Convert `pandas` -> ðŸ¤— `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good. How are things at the library?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michael</td>\n",
       "      <td>So you've come to the master for guidance? Is this what you're saying, grasshopper?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>michael</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54257</th>\n",
       "      <td>kevin</td>\n",
       "      <td>No, but maybe the reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>oscar</td>\n",
       "      <td>You're not gay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>erin</td>\n",
       "      <td>How did you do it? How did you capture what it was really like? How we felt and how made each other laugh and how we got through the day? How did you do it? Also, how do cameras work?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54265</th>\n",
       "      <td>jim</td>\n",
       "      <td>I sold paper at this company for 12 years. My job was to speak to clients on the phone about quantities and types of copier paper. Even if I didn't love every minute of it, everything I have, I owe to this job. This stupid...wonderful...boring...amazing job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54266</th>\n",
       "      <td>pam</td>\n",
       "      <td>I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39420 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker  \\\n",
       "0      michael   \n",
       "1          jim   \n",
       "2      michael   \n",
       "3          jim   \n",
       "4      michael   \n",
       "...        ...   \n",
       "54257    kevin   \n",
       "54258    oscar   \n",
       "54260     erin   \n",
       "54265      jim   \n",
       "54266      pam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     line  \n",
       "0                                                                                                                                                                                          All right Jim. Your quarterlies look very good. How are things at the library?  \n",
       "1                                                                                                                                                                                                                              Oh, I told you. I couldn't close it. So...  \n",
       "2                                                                                                                                                                                     So you've come to the master for guidance? Is this what you're saying, grasshopper?  \n",
       "3                                                                                                                                                                                                                              Actually, you called me in here, but yeah.  \n",
       "4                                                                                                                                                                                                                         All right. Well, let me show you how it's done.  \n",
       "...                                                                                                                                                                                                                                                                   ...  \n",
       "54257                                                                                                                                                                                                                                         No, but maybe the reason...  \n",
       "54258                                                                                                                                                                                                                                                     You're not gay.  \n",
       "54260                                                                             How did you do it? How did you capture what it was really like? How we felt and how made each other laugh and how we got through the day? How did you do it? Also, how do cameras work?  \n",
       "54265  I sold paper at this company for 12 years. My job was to speak to clients on the phone about quantities and types of copier paper. Even if I didn't love every minute of it, everything I have, I owe to this job. This stupid...wonderful...boring...amazing job.  \n",
       "54266            I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?  \n",
       "\n",
       "[39420 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit to top 10 most frequent speakers\n",
    "top_10_speaker_list = script_df['speaker'].value_counts(normalize=True).nlargest(10).index.tolist()\n",
    "columns_to_keep = ['speaker', 'line']\n",
    "\n",
    "script_df_subset = script_df.loc[script_df['speaker'].isin(top_10_speaker_list), columns_to_keep]\n",
    "\n",
    "script_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the 'line' column to be 'text'\n",
    "script_df_subset = script_df_subset.rename(columns={'line': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32668\n",
       "1     6752\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class label column\n",
    "dwight_mask = (script_df_subset['speaker'] == 'dwight')\n",
    "\n",
    "# new column of zeros\n",
    "script_df_subset['label'] = 0\n",
    "\n",
    "# apply the Dwight mask (as seen in the CPR scene of S05E14 \"Stress Relief\")\n",
    "script_df_subset.loc[dwight_mask, 'label'] = 1\n",
    "\n",
    "# adjust dtype\n",
    "script_df_subset['label'] = script_df_subset['label'].astype('int8')    \n",
    "    # would love to use 'category', but not implemented in ðŸ¤— datasets\n",
    "\n",
    "# check results\n",
    "script_df_subset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39420 entries, 0 to 54266\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   speaker  39420 non-null  string\n",
      " 1   text     39420 non-null  string\n",
      " 2   label    39420 non-null  int8  \n",
      "dtypes: int8(1), string(2)\n",
      "memory usage: 7.0 MB\n"
     ]
    }
   ],
   "source": [
    "script_df_subset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0de44b581d4d0c94c6381313763d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/39420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finally, convert to ðŸ¤— dataset object\n",
    "#   drop 'speaker' by way of not including it\n",
    "dataset_full: Dataset = Dataset.from_pandas(script_df_subset[['text', 'label']].reset_index(drop=False)) \\\n",
    "                    .cast_column('label', ClassLabel(names=['not_dwight', 'dwight']))\n",
    "\n",
    "# make sure we got the class labels mapped correctly\n",
    "assert (dataset_full.features['label'].str2int('dwight') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'label'],\n",
       "    num_rows: 39420\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Train/Test/Val Split\n",
    "\n",
    "As of v2.12.0, the ðŸ¤— Datasets implementation of `train_test_split` is limited to outputting **two** splits only (train/test), so we'll perform the split twice to obtain train, test, and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 19710\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 9855\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 9855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters\n",
    "train_size = 0.50\n",
    "test_size = 0.25\n",
    "valid_size = 0.25\n",
    "\n",
    "assert sum([train_size, test_size, valid_size]) == 1.0\n",
    "\n",
    "split_random_seed = 27  # for Weird Al fans\n",
    "\n",
    "first_split = dataset_full.train_test_split(\n",
    "    test_size=(1.0 - train_size),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "second_split = first_split['test'].train_test_split(\n",
    "    test_size=((valid_size) / (test_size + valid_size)),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "ds_dict = DatasetDict({\n",
    "    'train': first_split['train'],\n",
    "    'test': second_split['train'],\n",
    "    'valid': second_split['test']\n",
    "})\n",
    "\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio positive/negative is:\t1 to 4.8\n"
     ]
    }
   ],
   "source": [
    "# confirm stratified sample\n",
    "num_negative = ds_dict['train'].to_pandas()['label'].value_counts()[0]\n",
    "num_positive = ds_dict['train'].to_pandas()['label'].value_counts()[1]\n",
    "\n",
    "print(f\"ratio positive/negative is:\\t1 to {num_negative/num_positive:0.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Tokenize and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb8f1725e824b019828d21512da067b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542edede91d64fbc87d5097921bf4a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d963242738a4ef7baec2f1920040dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], \n",
    "                     padding='longest', \n",
    "                     truncation=True, \n",
    "                     return_tensors='pt',\n",
    "                     max_length=128)\n",
    "\n",
    "ds_tokenized = ds_dict.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original text:\n",
      "\t Birthday time is over! Now go make up for all the work you missed when you were taking your nap.  Many happy returns. \n",
      "\n",
      "Label:\t1\n",
      "\n",
      "Tokenized form:\n",
      "\t[CLS] birthday time is over ! now go make up for all the work you missed when you were taking your nap . many happy returns . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Tokens as a list:\n",
      "\t['[CLS]', 'birthday', 'time', 'is', 'over', '!', 'now', 'go', 'make', 'up', 'for', 'all', 'the', 'work', 'you', 'missed', 'when', 'you', 'were', 'taking', 'your', 'nap', '.', 'many', 'happy', 'returns', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['[CLS]', 'birthday', 'time', 'is', 'over', '!', 'now', 'go', 'make', 'up', 'for', 'all', 'the', 'work', 'you', 'missed', 'when', 'you', 'were', 'taking', 'your', 'nap', '.', 'many', 'happy', 'returns', '.', '[SEP]']\n",
      "\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "\tBy 2:00, Dwight will chose himself to be assistant to his own assistant, me.\n",
      "\n",
      "Label:\t0\n",
      "\n",
      "Tokenized form:\n",
      "\t[CLS] by 2 : 00 , dwight will chose himself to be assistant to his own assistant , me . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Tokens as a list:\n",
      "\t['[CLS]', 'by', '2', ':', '00', ',', 'dwight', 'will', 'chose', 'himself', 'to', 'be', 'assistant', 'to', 'his', 'own', 'assistant', ',', 'me', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['[CLS]', 'by', '2', ':', '00', ',', 'dwight', 'will', 'chose', 'himself', 'to', 'be', 'assistant', 'to', 'his', 'own', 'assistant', ',', 'me', '.', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_tokens(tokenizer, ds_tokenized['train'][27])\n",
    "inspect_tokens(tokenizer, ds_tokenized['test'][42])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Model\n",
    "\n",
    "Create model from pre-trained ðŸ¤— transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=2,\n",
    "    id2label={idx: label for idx, label in enumerate(ds_dict['train'].features['label'].names)}\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp.now().strftime(r'%Y%m%d_%H%M%S')  # yyyymmdd_hhmmss\n",
    "run_name = f\"basic_distilbert_{start_time}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # model output\n",
    "    run_name=run_name,\n",
    "    output_dir=MODEL_DIR / run_name,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    # training hyperparams\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation during training\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    log_level='warning',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training / evaluation metric\n",
    "#   Docs: https://huggingface.co/docs/evaluate/package_reference/main_classes#evaluate.combine\n",
    "#   Each of these metrics corresponds to a script from huggingface, below are the links for each script.\n",
    "#       accuracy:       https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "#       f1:             https://huggingface.co/spaces/evaluate-metric/f1\n",
    "#       precision:      https://huggingface.co/spaces/evaluate-metric/precision\n",
    "#       recall:         https://huggingface.co/spaces/evaluate-metric/recall\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "metric = evaluate.combine(evaluations=metric_list)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, setup and run the ðŸ¤— Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jminn\\.envs\\ds_env\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4b8d6ed8454a1f94c754d0a0cbe8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4387, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16a757fafba4bca86afd9ba2a909095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4204602539539337, 'eval_accuracy': 0.8330796549974632, 'eval_f1': 0.0996168582375479, 'eval_precision': 0.6546762589928058, 'eval_recall': 0.05390995260663507, 'eval_runtime': 17.452, 'eval_samples_per_second': 564.692, 'eval_steps_per_second': 17.648, 'epoch': 1.0}\n",
      "{'loss': 0.3663, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fe5da0a28440b69e6f610db7ba2055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43789076805114746, 'eval_accuracy': 0.8310502283105022, 'eval_f1': 0.30884184308841844, 'eval_precision': 0.5159500693481276, 'eval_recall': 0.22037914691943128, 'eval_runtime': 17.4079, 'eval_samples_per_second': 566.123, 'eval_steps_per_second': 17.693, 'epoch': 2.0}\n",
      "{'loss': 0.2701, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f263f950c0824a538cdf0a7b12d0961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5542342662811279, 'eval_accuracy': 0.8284119736174531, 'eval_f1': 0.21531322505800468, 'eval_precision': 0.49678800856531047, 'eval_recall': 0.13744075829383887, 'eval_runtime': 17.614, 'eval_samples_per_second': 559.497, 'eval_steps_per_second': 17.486, 'epoch': 3.0}\n",
      "{'loss': 0.1967, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55a4ba33c5c4be9ae9673a50c013b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6433358788490295, 'eval_accuracy': 0.812886859462202, 'eval_f1': 0.3366906474820144, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.2772511848341232, 'eval_runtime': 17.4646, 'eval_samples_per_second': 564.284, 'eval_steps_per_second': 17.636, 'epoch': 4.0}\n",
      "{'loss': 0.1501, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3bed8ac1124dd197158a8009eaf145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7656663060188293, 'eval_accuracy': 0.8100456621004566, 'eval_f1': 0.3304721030042918, 'eval_precision': 0.41696750902527074, 'eval_recall': 0.273696682464455, 'eval_runtime': 17.5101, 'eval_samples_per_second': 562.819, 'eval_steps_per_second': 17.59, 'epoch': 5.0}\n",
      "{'train_runtime': 587.8245, 'train_samples_per_second': 167.652, 'train_steps_per_second': 5.24, 'train_loss': 0.28436563417509003, 'epoch': 5.0}\n",
      "\n",
      "Training duration: 0 days 00:09:48.068031\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized['train'],\n",
    "    eval_dataset=ds_tokenized['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(\"\\nTraining duration:\", str(time_training))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()    # saves to self.args.output_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f400b537aa0403f82c8c25c51d62516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e079b6a1c984621aaed7a3e29783e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff32865a6e6449dbbac67fb2ddd5eed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation duration, what's the situation: 0 days 00:01:12.892828\n"
     ]
    }
   ],
   "source": [
    "time_evaluation_start = pd.Timestamp.now()\n",
    "\n",
    "final_metrics = {}\n",
    "final_metrics['train'] = trainer.evaluate(eval_dataset=ds_tokenized['train'], metric_key_prefix='final_train')\n",
    "final_metrics['test']= trainer.evaluate(eval_dataset=ds_tokenized['test'], metric_key_prefix='final_test')\n",
    "final_metrics['valid'] = trainer.evaluate(eval_dataset=ds_tokenized['valid'], metric_key_prefix='validation')\n",
    "\n",
    "time_evaluation_stop = pd.Timestamp.now()\n",
    "time_evaluation = time_evaluation_stop - time_evaluation_start\n",
    "\n",
    "print(\"\\nEvaluation duration, what's the situation:\", str(time_evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----TRAIN---------------\n",
      "     0.113 - final_train_loss\n",
      "     0.961 - final_train_accuracy\n",
      "     0.875 - final_train_f1\n",
      "     0.978 - final_train_precision\n",
      "     0.791 - final_train_recall\n",
      "    36.321 - final_train_runtime\n",
      "   542.667 - final_train_samples_per_second\n",
      "    16.960 - final_train_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "------TEST---------------\n",
      "     0.766 - final_test_loss\n",
      "     0.810 - final_test_accuracy\n",
      "     0.330 - final_test_f1\n",
      "     0.417 - final_test_precision\n",
      "     0.274 - final_test_recall\n",
      "    18.720 - final_test_runtime\n",
      "   526.447 - final_test_samples_per_second\n",
      "    16.453 - final_test_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "-----VALID---------------\n",
      "     0.775 - validation_loss\n",
      "     0.806 - validation_accuracy\n",
      "     0.322 - validation_f1\n",
      "     0.402 - validation_precision\n",
      "     0.268 - validation_recall\n",
      "    17.825 - validation_runtime\n",
      "   552.862 - validation_samples_per_second\n",
      "    17.279 - validation_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for split in final_metrics:\n",
    "    print(f\"\\n{split.upper():->10}{'-'*15}\")\n",
    "    for k, v in final_metrics[split].items():\n",
    "        print(f\"{v:>10.3f} - {k}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 - Discussion / Conclusions (on this attempt)\n",
    "\n",
    "| Metrics (Train/Test/Valid)         | Accuracy              | F1 Score              | Precision             | Recall                | Fine-Tuning Time |\n",
    "|------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|------------------|\n",
    "| (1) Basic Transformer (DistilBERT) | 0.961 / 0.810 / 0.806 | 0.875 / 0.330 / 0.322 | 0.978 / 0.417 / 0.402 | 0.791 / 0.274 / 0.268 | 0d 0h 9m 48s     |\n",
    "\n",
    "### 1.6.1 - Accuracy\n",
    "Because of the class imbalance present in this run (1 positive to ~4.8 negative), we wanted to be sure to compare against the trivial classifier, i.e. \"always call 'negative'\". This trivial classifier would have an accuracy of 82.9% ( $\\frac{32,668}{32,668+6,752}$ ). Based on this, the test and validation accuracies *less than* 82.9% are suggesting the model is not performing better than the trivial classifier.\n",
    "\n",
    "### 1.6.2 - F1 Score\n",
    "The final F1 scores are significantly different between the training set and the test/validation set, and though the test F1 score did show some improvement over the training epochs, it does not appear to be stable (at least at the per-epoch resolution shown).\n",
    "\n",
    "### 1.6.3 - Precision / Recall\n",
    "For this attempt and its Dwight/Not-Dwight task, we would likely weigh the importance of precision and recall as approximately equivalent, with a slight advantage to precision:\n",
    "\n",
    " - Higher precision is indicative of fewer false **positives**, more true positives, or both. While a true positive is intuitively desirable, a false positive (labeling a line's speaker as \"Dwight\" when the speaker is not Dwight) would be confusing to the end-user of the speaker labels.\n",
    " - Higher recall is indicative of fewer false **negatives**, more true positives, or both. A false negative in this case would represent a line spoken by Dwight being labeled as \"Not-Dwight\". Because our task is not attempting to classify further than \"Not-Dwight\", e.g. to identify which Not-Dwight speaker uttered the line, false negatives would more likely signal the need for further analysis. While outside the scope of this attempt, the downstream analysis could take the form of an ensemble approach to apply other modeling techniques (e.g. boosting) to the difficult-to-classify lines.\n",
    "\n",
    "In short, false positives are clearly detrimental, while false negatives *may* be detrimental if downstream modeling can't differentiate the speaker. This supports the prior assertion that precision has a small advantage over recall for importance to our anaysis. This also suggests that F1 score (noted above) is a valuable metric here, as it encapsulates both precision and recall.\n",
    "\n",
    "Turning to the results of this attempt: for both precision and recall, like what was seen in F1 score the training versus test/eval performance again show a stark difference, suggesting the model is not generalizing well.\n",
    "\n",
    "### 1.6.4 - Overall \"Basic Transformer\" Conclusion\n",
    "We would not call this a successful model, most notably for its performance metrics falling significantly lower in the the test/validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Modified Approach: Different Pretrained Model (RoBERTa)\n",
    "\n",
    "We'll attempt to improve our model performance by starting with a \"robustly optimized\" (hence the \"Ro\") pretrained model, RoBERTa. Most of the details below will be kept the same for experimental control.\n",
    "\n",
    "> NOTE: Differences from the \"Basic Transformer\" (Section 1) are noted with \"`>>`\" chevrons.\n",
    "\n",
    "**Task**: Sequence Classification (Binary)\n",
    "\n",
    "**Classes**: \n",
    " - Positive (1): \"Dwight\" - a line is spoken by the character Dwight K. Schrute (played by Rainn Wilson).\n",
    " - Negative (0): \"Not Dwight\" - a line is spoken by any other character than Dwight.\n",
    "\n",
    "**Data**:\n",
    " - `speaker` as pre-cursor to class label. Limited to top-10 most frequent speakers based on number of lines in dataset\n",
    " - `line` as sequence text.\n",
    "\n",
    "**Encoding**:\n",
    " - `>>` Tokenizer: RobertaTokenizerFast\n",
    " - Max Sequence Length: 128\n",
    " - Padding: True\n",
    " - Truncate: True\n",
    "\n",
    "**Pretrained Model**:\n",
    " - `>>` RoBERTa (`roberta-base`) [(link: huggingface.co)](https://huggingface.co/roberta-base) - Intended to improve upon BERTbase with refined pretraining hyperparameters and a larger pretraining text corpus.\n",
    " - `>>` Citation: Liu et al. \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" (2019) - [https://arxiv.org/pdf/1907.11692.pdf](https://arxiv.org/pdf/1907.11692.pdf)\n",
    "\n",
    "**Training**:\n",
    " - Train/Test/Validation Split: 50/25/25\n",
    "\n",
    "**Notes**:\n",
    " - Class imbalance is present (positive: 6,752; negative: 32,668; about `1:4.8` imbalance ratio).\n",
    " - Vocabulary: no modifications made to pretrained transformer's vocabulary.\n",
    " - Secondary data: no inclusion of secondary data (director/writer credits)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Dataset - Convert `pandas` -> ðŸ¤— `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good. How are things at the library?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michael</td>\n",
       "      <td>So you've come to the master for guidance? Is this what you're saying, grasshopper?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim</td>\n",
       "      <td>Actually, you called me in here, but yeah.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>michael</td>\n",
       "      <td>All right. Well, let me show you how it's done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54257</th>\n",
       "      <td>kevin</td>\n",
       "      <td>No, but maybe the reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54258</th>\n",
       "      <td>oscar</td>\n",
       "      <td>You're not gay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54260</th>\n",
       "      <td>erin</td>\n",
       "      <td>How did you do it? How did you capture what it was really like? How we felt and how made each other laugh and how we got through the day? How did you do it? Also, how do cameras work?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54265</th>\n",
       "      <td>jim</td>\n",
       "      <td>I sold paper at this company for 12 years. My job was to speak to clients on the phone about quantities and types of copier paper. Even if I didn't love every minute of it, everything I have, I owe to this job. This stupid...wonderful...boring...amazing job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54266</th>\n",
       "      <td>pam</td>\n",
       "      <td>I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39420 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker  \\\n",
       "0      michael   \n",
       "1          jim   \n",
       "2      michael   \n",
       "3          jim   \n",
       "4      michael   \n",
       "...        ...   \n",
       "54257    kevin   \n",
       "54258    oscar   \n",
       "54260     erin   \n",
       "54265      jim   \n",
       "54266      pam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     line  \n",
       "0                                                                                                                                                                                          All right Jim. Your quarterlies look very good. How are things at the library?  \n",
       "1                                                                                                                                                                                                                              Oh, I told you. I couldn't close it. So...  \n",
       "2                                                                                                                                                                                     So you've come to the master for guidance? Is this what you're saying, grasshopper?  \n",
       "3                                                                                                                                                                                                                              Actually, you called me in here, but yeah.  \n",
       "4                                                                                                                                                                                                                         All right. Well, let me show you how it's done.  \n",
       "...                                                                                                                                                                                                                                                                   ...  \n",
       "54257                                                                                                                                                                                                                                         No, but maybe the reason...  \n",
       "54258                                                                                                                                                                                                                                                     You're not gay.  \n",
       "54260                                                                             How did you do it? How did you capture what it was really like? How we felt and how made each other laugh and how we got through the day? How did you do it? Also, how do cameras work?  \n",
       "54265  I sold paper at this company for 12 years. My job was to speak to clients on the phone about quantities and types of copier paper. Even if I didn't love every minute of it, everything I have, I owe to this job. This stupid...wonderful...boring...amazing job.  \n",
       "54266            I thought it was weird when you picked us to make a documentary. But all in all...I think an ordinary paper company like Dunder Mifflin was a great subject for a documentary. There's a lot of beauty in ordinary things. Isn't that kind of the point?  \n",
       "\n",
       "[39420 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit to top 10 most frequent speakers\n",
    "top_10_speaker_list = script_df['speaker'].value_counts(normalize=True).nlargest(10).index.tolist()\n",
    "columns_to_keep = ['speaker', 'line']\n",
    "\n",
    "script_df_subset = script_df.loc[script_df['speaker'].isin(top_10_speaker_list), columns_to_keep]\n",
    "\n",
    "script_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the 'line' column to be 'text'\n",
    "script_df_subset = script_df_subset.rename(columns={'line': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32668\n",
       "1     6752\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class label column\n",
    "dwight_mask = (script_df_subset['speaker'] == 'dwight')\n",
    "\n",
    "# new column of zeros\n",
    "script_df_subset['label'] = 0\n",
    "\n",
    "# apply the Dwight mask (as seen in the CPR scene of S05E14 \"Stress Relief\")\n",
    "script_df_subset.loc[dwight_mask, 'label'] = 1\n",
    "\n",
    "# adjust dtype\n",
    "script_df_subset['label'] = script_df_subset['label'].astype('int8')    \n",
    "    # would love to use 'category', but not implemented in ðŸ¤— datasets\n",
    "\n",
    "# check results\n",
    "script_df_subset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39420 entries, 0 to 54266\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   speaker  39420 non-null  string\n",
      " 1   text     39420 non-null  string\n",
      " 2   label    39420 non-null  int8  \n",
      "dtypes: int8(1), string(2)\n",
      "memory usage: 7.0 MB\n"
     ]
    }
   ],
   "source": [
    "script_df_subset.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf72fa6e000a4a38987f34a6a8c16410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/39420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finally, convert to ðŸ¤— dataset object\n",
    "#   drop 'speaker' by way of not including it\n",
    "dataset_full: Dataset = Dataset.from_pandas(script_df_subset[['text', 'label']].reset_index(drop=False)) \\\n",
    "                    .cast_column('label', ClassLabel(names=['not_dwight', 'dwight']))\n",
    "\n",
    "# make sure we got the class labels mapped correctly\n",
    "assert (dataset_full.features['label'].str2int('dwight') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'label'],\n",
       "    num_rows: 39420\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Train/Test/Val Split\n",
    "\n",
    "As of v2.12.0, the ðŸ¤— Datasets implementation of `train_test_split` is limited to outputting **two** splits only (train/test), so we'll perform the split twice to obtain train, test, and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 19710\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 9855\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 9855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters\n",
    "train_size = 0.50\n",
    "test_size = 0.25\n",
    "valid_size = 0.25\n",
    "\n",
    "assert sum([train_size, test_size, valid_size]) == 1.0\n",
    "\n",
    "split_random_seed = 27  # for Weird Al fans\n",
    "\n",
    "first_split = dataset_full.train_test_split(\n",
    "    test_size=(1.0 - train_size),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "second_split = first_split['test'].train_test_split(\n",
    "    test_size=((valid_size) / (test_size + valid_size)),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "ds_dict = DatasetDict({\n",
    "    'train': first_split['train'],\n",
    "    'test': second_split['train'],\n",
    "    'valid': second_split['test']\n",
    "})\n",
    "\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio positive/negative is:\t1 to 4.8\n"
     ]
    }
   ],
   "source": [
    "# confirm stratified sample\n",
    "num_negative = ds_dict['train'].to_pandas()['label'].value_counts()[0]\n",
    "num_positive = ds_dict['train'].to_pandas()['label'].value_counts()[1]\n",
    "\n",
    "print(f\"ratio positive/negative is:\\t1 to {num_negative/num_positive:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_dwight', 'dwight']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict['train'].features['label'].names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Tokenize and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e317a124544c229a56cb79aa1236d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652f9059b9694550bf2cfb2ce22b82d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464fa00e8dd42cfb4d92f92e482c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], \n",
    "                     padding='longest', \n",
    "                     truncation=True, \n",
    "                     return_tensors='pt',\n",
    "                     #max_length=128\n",
    "                     )\n",
    "\n",
    "ds_tokenized = ds_dict.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original text:\n",
      "\t Birthday time is over! Now go make up for all the work you missed when you were taking your nap.  Many happy returns. \n",
      "\n",
      "Label:\t1\n",
      "\n",
      "Tokenized form:\n",
      "\t<s> Ä Birthday Ä time Ä is Ä over ! Ä Now Ä go Ä make Ä up Ä for Ä all Ä the Ä work Ä you Ä missed Ä when Ä you Ä were Ä taking Ä your Ä nap . Ä  Ä Many Ä happy Ä returns . Ä  </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Tokens as a list:\n",
      "\t['<s>', 'Ä Birthday', 'Ä time', 'Ä is', 'Ä over', '!', 'Ä Now', 'Ä go', 'Ä make', 'Ä up', 'Ä for', 'Ä all', 'Ä the', 'Ä work', 'Ä you', 'Ä missed', 'Ä when', 'Ä you', 'Ä were', 'Ä taking', 'Ä your', 'Ä nap', '.', 'Ä ', 'Ä Many', 'Ä happy', 'Ä returns', '.', 'Ä ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['<s>', 'Ä Birthday', 'Ä time', 'Ä is', 'Ä over', '!', 'Ä Now', 'Ä go', 'Ä make', 'Ä up', 'Ä for', 'Ä all', 'Ä the', 'Ä work', 'Ä you', 'Ä missed', 'Ä when', 'Ä you', 'Ä were', 'Ä taking', 'Ä your', 'Ä nap', '.', 'Ä ', 'Ä Many', 'Ä happy', 'Ä returns', '.', 'Ä ', '</s>']\n",
      "\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "\tBy 2:00, Dwight will chose himself to be assistant to his own assistant, me.\n",
      "\n",
      "Label:\t0\n",
      "\n",
      "Tokenized form:\n",
      "\t<s> By Ä 2 : 00 , Ä Dwight Ä will Ä chose Ä himself Ä to Ä be Ä assistant Ä to Ä his Ä own Ä assistant , Ä me . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "Tokens as a list:\n",
      "\t['<s>', 'By', 'Ä 2', ':', '00', ',', 'Ä Dwight', 'Ä will', 'Ä chose', 'Ä himself', 'Ä to', 'Ä be', 'Ä assistant', 'Ä to', 'Ä his', 'Ä own', 'Ä assistant', ',', 'Ä me', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['<s>', 'By', 'Ä 2', ':', '00', ',', 'Ä Dwight', 'Ä will', 'Ä chose', 'Ä himself', 'Ä to', 'Ä be', 'Ä assistant', 'Ä to', 'Ä his', 'Ä own', 'Ä assistant', ',', 'Ä me', '.', '</s>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_tokens(tokenizer, ds_tokenized['train'][27])\n",
    "inspect_tokens(tokenizer, ds_tokenized['test'][42])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Model\n",
    "\n",
    "Create model from pre-trained ðŸ¤— transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', \n",
    "    num_labels=2,\n",
    "    id2label={idx: label for idx, label in enumerate(ds_dict['train'].features['label'].names)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"not_dwight\",\n",
       "    \"1\": \"dwight\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.27.4\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp.now().strftime(r'%Y%m%d_%H%M%S')  # yyyymmdd_hhmmss\n",
    "run_name = f\"basic_roberta_{start_time}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # model output\n",
    "    run_name=run_name,\n",
    "    output_dir=MODEL_DIR / run_name,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    # training hyperparams\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation during training\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    log_level='warning',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training / evaluation metric\n",
    "#   Docs: https://huggingface.co/docs/evaluate/package_reference/main_classes#evaluate.combine\n",
    "#   Each of these metrics corresponds to a script from huggingface, below are the links for each script.\n",
    "#       accuracy:       https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "#       f1:             https://huggingface.co/spaces/evaluate-metric/f1\n",
    "#       precision:      https://huggingface.co/spaces/evaluate-metric/precision\n",
    "#       recall:         https://huggingface.co/spaces/evaluate-metric/recall\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "metric = evaluate.combine(evaluations=metric_list)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, setup and run the ðŸ¤— Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jminn\\.envs\\ds_env\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49590ef049a4c72b9b369929db523cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.463, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501eb4bff2c04763b147d417c0028b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43414580821990967, 'eval_accuracy': 0.8292237442922374, 'eval_f1': 0.015213575190169687, 'eval_precision': 0.6190476190476191, 'eval_recall': 0.007701421800947867, 'eval_runtime': 83.2058, 'eval_samples_per_second': 118.441, 'eval_steps_per_second': 3.702, 'epoch': 1.0}\n",
      "{'loss': 0.4257, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8fe6fc709e421baca464a97c0e3c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.430319219827652, 'eval_accuracy': 0.8289193302891933, 'eval_f1': 0.29396984924623115, 'eval_precision': 0.5014285714285714, 'eval_recall': 0.2079383886255924, 'eval_runtime': 82.5001, 'eval_samples_per_second': 119.454, 'eval_steps_per_second': 3.733, 'epoch': 2.0}\n",
      "{'loss': 0.3791, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31fade56fdf4d0393217bee610d7dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43349748849868774, 'eval_accuracy': 0.8347031963470319, 'eval_f1': 0.20497803806734993, 'eval_precision': 0.5817174515235457, 'eval_recall': 0.12440758293838862, 'eval_runtime': 83.2959, 'eval_samples_per_second': 118.313, 'eval_steps_per_second': 3.698, 'epoch': 3.0}\n",
      "{'loss': 0.325, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068c50b32de1475a9337d6694f77b34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47166863083839417, 'eval_accuracy': 0.8259766615930999, 'eval_f1': 0.2968429684296843, 'eval_precision': 0.48202396804260983, 'eval_recall': 0.21445497630331753, 'eval_runtime': 81.369, 'eval_samples_per_second': 121.115, 'eval_steps_per_second': 3.785, 'epoch': 4.0}\n",
      "{'loss': 0.2817, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2c6ca72144407c9431cacd8e9d0159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49859941005706787, 'eval_accuracy': 0.8242516489091831, 'eval_f1': 0.31378763866877973, 'eval_precision': 0.47368421052631576, 'eval_recall': 0.23459715639810427, 'eval_runtime': 81.8029, 'eval_samples_per_second': 120.472, 'eval_steps_per_second': 3.765, 'epoch': 5.0}\n",
      "{'train_runtime': 2722.1736, 'train_samples_per_second': 36.203, 'train_steps_per_second': 0.283, 'train_loss': 0.3748962501426796, 'epoch': 5.0}\n",
      "\n",
      "Training duration: 0 days 00:45:22.492777\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized['train'],\n",
    "    eval_dataset=ds_tokenized['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(\"\\nTraining duration:\", str(time_training))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()    # saves to self.args.output_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3605300c20f4e9fbb9854687a3c65b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca66cfbd894d403cb209a71307741861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b932577ead6b4e0d8411bc0594514bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation duration, what's the situation: 0 days 00:04:24.722081\n"
     ]
    }
   ],
   "source": [
    "time_evaluation_start = pd.Timestamp.now()\n",
    "\n",
    "final_metrics = {}\n",
    "final_metrics['train'] = trainer.evaluate(eval_dataset=ds_tokenized['train'], metric_key_prefix='final_train')\n",
    "final_metrics['test']= trainer.evaluate(eval_dataset=ds_tokenized['test'], metric_key_prefix='final_test')\n",
    "final_metrics['valid'] = trainer.evaluate(eval_dataset=ds_tokenized['valid'], metric_key_prefix='validation')\n",
    "\n",
    "time_evaluation_stop = pd.Timestamp.now()\n",
    "time_evaluation = time_evaluation_stop - time_evaluation_start\n",
    "\n",
    "print(\"\\nEvaluation duration, what's the situation:\", str(time_evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----TRAIN---------------\n",
      "     0.221 - final_train_loss\n",
      "     0.917 - final_train_accuracy\n",
      "     0.703 - final_train_f1\n",
      "     0.909 - final_train_precision\n",
      "     0.573 - final_train_recall\n",
      "   117.271 - final_train_runtime\n",
      "   168.072 - final_train_samples_per_second\n",
      "     5.253 - final_train_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "------TEST---------------\n",
      "     0.499 - final_test_loss\n",
      "     0.824 - final_test_accuracy\n",
      "     0.314 - final_test_f1\n",
      "     0.474 - final_test_precision\n",
      "     0.235 - final_test_recall\n",
      "    82.584 - final_test_runtime\n",
      "   119.333 - final_test_samples_per_second\n",
      "     3.730 - final_test_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "-----VALID---------------\n",
      "     0.495 - validation_loss\n",
      "     0.825 - validation_accuracy\n",
      "     0.324 - validation_f1\n",
      "     0.480 - validation_precision\n",
      "     0.245 - validation_recall\n",
      "    64.822 - validation_runtime\n",
      "   152.032 - validation_samples_per_second\n",
      "     4.751 - validation_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for split in final_metrics:\n",
    "    print(f\"\\n{split.upper():->10}{'-'*15}\")\n",
    "    for k, v in final_metrics[split].items():\n",
    "        print(f\"{v:>10.3f} - {k}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 - Informal Test\n",
    "\n",
    "Feeding two completely made-up lines to the fine-tuned model, mostly for fun but also as a small test of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "         Test Line:  \"Assistant to the regional manager of beets, Mose and mother on the farm\"\n",
      " Predicted Speaker:  dwight\n",
      "            Logits:  tensor([[-1.8353,  2.3127]], device='cuda:0')\n",
      "--------------------------------------------------\n",
      "         Test Line:  \"My name is Michael Scott, paper is my business\"\n",
      " Predicted Speaker:  not_dwight\n",
      "            Logits:  tensor([[ 1.7975, -1.3024]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_lines = [\n",
    "    \"Assistant to the regional manager of beets, Mose and mother on the farm\",\n",
    "    \"My name is Michael Scott, paper is my business\",\n",
    "]\n",
    "\n",
    "for line in test_lines:\n",
    "    print(\"-\"*50)\n",
    "    informal_test(tokenizer, model, line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Discussion / Conclusions (on this attempt)\n",
    "\n",
    "| Metrics (Train/Test/Valid)         | Accuracy              | F1 Score              | Precision             | Recall                | Fine-Tuning Time |\n",
    "|------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|------------------|\n",
    "| (1) Basic Transformer (DistilBERT) | 0.961 / 0.810 / 0.806 | 0.875 / 0.330 / 0.322 | 0.978 / 0.417 / 0.402 | 0.791 / 0.274 / 0.268 | 0d 0h 9m 48s     |\n",
    "| (2) Mod: Different PLM (RoBERTa)   | 0.917 / 0.824 / 0.825 | 0.703 / 0.314 / 0.324 | 0.909 / 0.474 / 0.480 | 0.573 / 0.235 / 0.245 | 0d 0h 45m 23s    |\n",
    "\n",
    "### 2.6.1 - Accuracy\n",
    "Accuracy for this attempt was at least (roughly) equivalent to the trivial classifier but still not worth celebrating.\n",
    "\n",
    "### 2.6.2 - F1 Score\n",
    "Interestingly the validation F1 score stayed approximately the same while the train+test F1 scores decreased. During training, the test F1 score still did not appear to be stable/converging over the five epochs. Training loss did steadily decrease so it may be possible the model is undertrained, but considering the higher training accuracy we would be concerned that further training would overfit the training data.\n",
    "\n",
    "### 2.6.3 - Precision / Recall\n",
    "Neither measure appeared stable during training, but there is some small improvement here for precision (likely at the cost of the apparent decrease in recall).\n",
    "\n",
    "### 2.6.4 - Overall \"Modified Approach: Different Pretrained Model (RoBERTa)\" Conclusion\n",
    "This modification (to swap in a more \"robustly optimized\" pretrained language model) has some improvement to precision but overall does not appear to end up as winning model for our task. The training time for fine-tuning was also increased by a factor of ~4.6 which is an expected but discouraging result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Modified Approach: Re-Balance Dataset\n",
    "\n",
    "Returning to our original pretrained model (DistilBERT), we'll attempt to make modifications to our dataset to try to realize some better classification performance.\n",
    "\n",
    "> NOTE: Differences from the \"Basic Transformer\" (Section 1) are noted with \"`>>`\" chevrons.\n",
    "\n",
    "**Task**: Sequence Classification (Binary)\n",
    "\n",
    "**Classes**: \n",
    " - Positive (1): \"Dwight\" - a line is spoken by the character Dwight K. Schrute (played by Rainn Wilson).\n",
    " - Negative (0): \"Not Dwight\" - a line is spoken by any other character than Dwight.\n",
    "\n",
    "**Data**:\n",
    " - `speaker` as pre-cursor to class label. Limited to top-10 most frequent speakers based on number of lines in dataset.\n",
    " - `line` as sequence text.\n",
    " - `>>` Class imbalance is addressed. Two techniques are applied (separately): (1) undersample the negative class; (2) oversample the positive class. More details under \"Notes\" below.\n",
    "\n",
    "**Encoding**:\n",
    " - Tokenizer: DistilBertTokenizerFast\n",
    " - Max Sequence Length: 128\n",
    " - Padding: True\n",
    " - Truncate: True\n",
    "\n",
    "**Pretrained Model**:\n",
    " - DistilBert (`distilbert-base-uncased`) [(link: huggingface.co)](https://huggingface.co/distilbert-base-uncased) - Intended to mimic the standard \"BERTbase\" model but in a smaller/faster/more efficient way.\n",
    " - Citation: Sanh et al. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" (2019) - [https://arxiv.org/pdf/1910.01108.pdf](https://arxiv.org/pdf/1910.01108.pdf)\n",
    "\n",
    "**Training**:\n",
    " - Train/Test/Validation Split: 50/25/25\n",
    "\n",
    "**Notes**:\n",
    " - `>>` Two approaches are attempted for re-balancing the training dataset.\n",
    "   - (1) Undersample the negative class - There are several ways we could choose to accomplish this.\n",
    "     - Choose a smaller N than $N=10$ of our \"top N speakers\" filter until the number of Not-Dwight utterances is closer to the number of Dwight utterances.\n",
    "     - Maintain the same $N=10$ approach but reduce the number of Non-Dwight utterances in the training dataset (i.e. \"use all of Dwight's utterances, but only use XX% of the Top-10-Speaker Non-Dwight utterances\").\n",
    "   - (2) Oversample the positive class\n",
    "     - If we had unlimited time, it would be interesting to use an approach like [Synthetic Minority Over-sampling Technique (SMOTE)](https://arxiv.org/abs/1106.1813), perhaps using numeric features derived from GloVe vectors to choose \"similar\" words to real Dwight lines.\n",
    "     - More within the realm of NLP, we could also consider using a generative language model to generate artificial lines until we have enough Dwight lines (real+artificial) to balance the dataset.\n",
    "   - For this first attempt at re-balancing, we'll go with a low-complexity approach: \n",
    "     - Double the positive class samples (2x oversample)\n",
    "     - Halve the negative class samples (2x undersample)\n",
    "     - This would bring us to a class balance of about 1:1.2 (positive to negative).\n",
    " - Vocabulary: no modifications made to pretrained transformer's vocabulary.\n",
    " - Secondary data: no inclusion of secondary data (director/writer credits)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32668\n",
      "1     6752\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>michael</td>\n",
       "      <td>That's, that is true.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17790</th>\n",
       "      <td>andy</td>\n",
       "      <td>All right!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44715</th>\n",
       "      <td>oscar</td>\n",
       "      <td>Un-be-liev-a-ble.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50914</th>\n",
       "      <td>dwight</td>\n",
       "      <td>We just need a pretense to talk to him. We could tell him that his mother is dying. That usually works on him.  Nate. Your mother is dying.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>dwight</td>\n",
       "      <td>I'm not.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>dwight</td>\n",
       "      <td>Do you have the tools to turn a wooden mop handle into a stake?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker  \\\n",
       "30002  michael   \n",
       "17790     andy   \n",
       "44715    oscar   \n",
       "50914   dwight   \n",
       "10118   dwight   \n",
       "11313   dwight   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "30002                                                                                                                        That's, that is true.    \n",
       "17790                                                                                                                                    All right!   \n",
       "44715                                                                                                                             Un-be-liev-a-ble.   \n",
       "50914  We just need a pretense to talk to him. We could tell him that his mother is dying. That usually works on him.  Nate. Your mother is dying.    \n",
       "10118                                                                                                                                      I'm not.   \n",
       "11313                                                                               Do you have the tools to turn a wooden mop handle into a stake?   \n",
       "\n",
       "       label  \n",
       "30002      0  \n",
       "17790      0  \n",
       "44715      0  \n",
       "50914      1  \n",
       "10118      1  \n",
       "11313      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit to top 10 most frequent speakers\n",
    "top_10_speaker_list = script_df['speaker'].value_counts(normalize=True).nlargest(10).index.tolist()\n",
    "columns_to_keep = ['speaker', 'line']\n",
    "\n",
    "script_df_subset = script_df.loc[script_df['speaker'].isin(top_10_speaker_list), columns_to_keep]\n",
    "\n",
    "# rename the 'line' column to be 'text'\n",
    "script_df_subset = script_df_subset.rename(columns={'line': 'text'})\n",
    "\n",
    "# create class label column\n",
    "dwight_mask = (script_df_subset['speaker'] == 'dwight')\n",
    "\n",
    "# new column of zeros\n",
    "script_df_subset['label'] = 0\n",
    "\n",
    "# apply the Dwight mask (as seen in the CPR scene of S05E14 \"Stress Relief\")\n",
    "script_df_subset.loc[dwight_mask, 'label'] = 1\n",
    "\n",
    "# adjust dtype\n",
    "script_df_subset['label'] = script_df_subset['label'].astype('int8')\n",
    "\n",
    "# check results\n",
    "print(script_df_subset['label'].value_counts())\n",
    "\n",
    "pd.concat(\n",
    "    [script_df_subset.loc[script_df_subset['label'] == 0].sample(3, random_state=42),\n",
    "     script_df_subset.loc[script_df_subset['label'] == 1].sample(3, random_state=42)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16334\n",
       "1    13504\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oversample positive class (double)\n",
    "positive_class = pd.concat([\n",
    "    script_df_subset.loc[script_df_subset['label'] == 1],\n",
    "    script_df_subset.loc[script_df_subset['label'] == 1]\n",
    "])\n",
    "\n",
    "# undersample negative class (half)\n",
    "negative_class = script_df_subset.loc[script_df_subset['label'] == 0].sample(frac=0.5, random_state=42)\n",
    "\n",
    "# recombine into original reference (to keep downstream code consistent)\n",
    "script_df_subset = pd.concat([\n",
    "    positive_class,\n",
    "    negative_class\n",
    "])\n",
    "\n",
    "# check results\n",
    "script_df_subset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adabfc9abc4749178502477b73df5f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/29838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'label'],\n",
       "    num_rows: 29838\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, convert to ðŸ¤— dataset object\n",
    "#   drop 'speaker' by way of not including it\n",
    "dataset_full: Dataset = Dataset.from_pandas(script_df_subset[['text', 'label']].reset_index(drop=False)) \\\n",
    "                    .cast_column('label', ClassLabel(names=['not_dwight', 'dwight']))\n",
    "\n",
    "# make sure we got the class labels mapped correctly\n",
    "assert (dataset_full.features['label'].str2int('dwight') == 1)\n",
    "\n",
    "# check results\n",
    "dataset_full"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Train/Test/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 14919\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 7459\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 7460\n",
      "    })\n",
      "})\n",
      "ratio positive/negative is:\t1 to 1.2\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "train_size = 0.50\n",
    "test_size = 0.25\n",
    "valid_size = 0.25\n",
    "\n",
    "assert sum([train_size, test_size, valid_size]) == 1.0\n",
    "\n",
    "split_random_seed = 27  # for Weird Al fans\n",
    "\n",
    "first_split = dataset_full.train_test_split(\n",
    "    test_size=(1.0 - train_size),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "second_split = first_split['test'].train_test_split(\n",
    "    test_size=((valid_size) / (test_size + valid_size)),\n",
    "    shuffle=True,\n",
    "    seed=split_random_seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "ds_dict = DatasetDict({\n",
    "    'train': first_split['train'],\n",
    "    'test': second_split['train'],\n",
    "    'valid': second_split['test']\n",
    "})\n",
    "\n",
    "print(ds_dict)\n",
    "\n",
    "# confirm stratified sample\n",
    "num_negative = ds_dict['train'].to_pandas()['label'].value_counts()[0]\n",
    "num_positive = ds_dict['train'].to_pandas()['label'].value_counts()[1]\n",
    "\n",
    "print(f\"ratio positive/negative is:\\t1 to {num_negative/num_positive:0.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Tokenize and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1387341d3a4115a77c58bb208ee625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2868690c78e4a319db8c967d87a80dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdad9b24e3d4f89b5086c28cd25e1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Original text:\n",
      "\tChair, lamp, plant, table leg, Jim's leg.\n",
      "\n",
      "Label:\t1\n",
      "\n",
      "Tokenized form:\n",
      "\t[CLS] chair , lamp , plant , table leg , jim ' s leg . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Tokens as a list:\n",
      "\t['[CLS]', 'chair', ',', 'lamp', ',', 'plant', ',', 'table', 'leg', ',', 'jim', \"'\", 's', 'leg', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['[CLS]', 'chair', ',', 'lamp', ',', 'plant', ',', 'table', 'leg', ',', 'jim', \"'\", 's', 'leg', '.', '[SEP]']\n",
      "\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "\tSo you fell in?\n",
      "\n",
      "Label:\t0\n",
      "\n",
      "Tokenized form:\n",
      "\t[CLS] so you fell in ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Tokens as a list:\n",
      "\t['[CLS]', 'so', 'you', 'fell', 'in', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Tokens as a list, attention mask applied:\n",
      "\t['[CLS]', 'so', 'you', 'fell', 'in', '?', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# tokenizer function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], \n",
    "                     padding='longest', \n",
    "                     truncation=True, \n",
    "                     return_tensors='pt',\n",
    "                     max_length=128\n",
    ")\n",
    "\n",
    "ds_tokenized = ds_dict.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=None\n",
    ")\n",
    "\n",
    "# note because of the over-/under-sampling, these test indices will reference \n",
    "#   different lines than the previous cases\n",
    "inspect_tokens(tokenizer, ds_tokenized['train'][27])\n",
    "inspect_tokens(tokenizer, ds_tokenized['test'][42])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=2,\n",
    "    id2label={idx: label for idx, label in enumerate(ds_dict['train'].features['label'].names)}\n",
    "    )\n",
    "\n",
    "start_time = pd.Timestamp.now().strftime(r'%Y%m%d_%H%M%S')  # yyyymmdd_hhmmss\n",
    "run_name = f\"basic_distilbert_{start_time}\"\n",
    "\n",
    "# setup training args\n",
    "training_args = TrainingArguments(\n",
    "    # model output\n",
    "    run_name=run_name,\n",
    "    output_dir=MODEL_DIR / run_name,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    # training hyperparams\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #gradient_checkpointing=True,\n",
    "    weight_decay=0.01,\n",
    "    # evaluation during training\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    log_level='warning',\n",
    ")\n",
    "\n",
    "# establish evaluation metrics:\n",
    "#   Docs: https://huggingface.co/docs/evaluate/package_reference/main_classes#evaluate.combine\n",
    "#   Each of these metrics corresponds to a script from huggingface, below are the links for each script.\n",
    "#       accuracy:       https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "#       f1:             https://huggingface.co/spaces/evaluate-metric/f1\n",
    "#       precision:      https://huggingface.co/spaces/evaluate-metric/precision\n",
    "#       recall:         https://huggingface.co/spaces/evaluate-metric/recall\n",
    "metric_list = ['accuracy', 'f1', 'precision', 'recall']\n",
    "\n",
    "metric = evaluate.combine(evaluations=metric_list)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jminn\\.envs\\ds_env\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707fe1c7cb03407881deec97c71b8a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.631, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f0cdd9aa7e40a2befb27fb40a2e63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5966618657112122, 'eval_accuracy': 0.6787773159941011, 'eval_f1': 0.5973109243697479, 'eval_precision': 0.6903651903651904, 'eval_recall': 0.5263625592417062, 'eval_runtime': 13.2932, 'eval_samples_per_second': 561.115, 'eval_steps_per_second': 17.603, 'epoch': 1.0}\n",
      "{'loss': 0.4779, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faaa562094543cb902aeabfdfdd21ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5742825269699097, 'eval_accuracy': 0.708808151226706, 'eval_f1': 0.6549094375595804, 'eval_precision': 0.7063056888279644, 'eval_recall': 0.6104857819905213, 'eval_runtime': 13.359, 'eval_samples_per_second': 558.349, 'eval_steps_per_second': 17.516, 'epoch': 2.0}\n",
      "{'loss': 0.3098, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f60b0b4a38454da99c8d7b6a16a5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6948241591453552, 'eval_accuracy': 0.728247754390669, 'eval_f1': 0.6928322473101984, 'eval_precision': 0.709277071051815, 'eval_recall': 0.6771327014218009, 'eval_runtime': 13.2743, 'eval_samples_per_second': 561.912, 'eval_steps_per_second': 17.628, 'epoch': 3.0}\n",
      "{'loss': 0.198, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0922305b67e4308b5db0d72794d1129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.844700038433075, 'eval_accuracy': 0.7334763373106314, 'eval_f1': 0.6945298094652735, 'eval_precision': 0.7215836526181354, 'eval_recall': 0.669431279620853, 'eval_runtime': 13.3109, 'eval_samples_per_second': 560.368, 'eval_steps_per_second': 17.58, 'epoch': 4.0}\n",
      "{'loss': 0.1421, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef27ea833f4049e39bac9a320716461c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.009962558746338, 'eval_accuracy': 0.7326719399383296, 'eval_f1': 0.7050295857988165, 'eval_precision': 0.7041962174940898, 'eval_recall': 0.7058649289099526, 'eval_runtime': 13.2854, 'eval_samples_per_second': 561.442, 'eval_steps_per_second': 17.613, 'epoch': 5.0}\n",
      "{'train_runtime': 446.8035, 'train_samples_per_second': 166.953, 'train_steps_per_second': 5.226, 'train_loss': 0.3517736626896664, 'epoch': 5.0}\n",
      "\n",
      "Training duration: 0 days 00:07:27.073711\n"
     ]
    }
   ],
   "source": [
    "time_training_start = pd.Timestamp.now()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized['train'],\n",
    "    eval_dataset=ds_tokenized['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "time_training_stop = pd.Timestamp.now()\n",
    "time_training = time_training_stop - time_training_start\n",
    "\n",
    "print(\"\\nTraining duration:\", str(time_training))\n",
    "\n",
    "# save the trained model:\n",
    "trainer.save_model()    # saves to self.args.output_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e87ce4c5bbc414e927ec5d55c9afb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb56554a63a14131bde1d84a6c63b81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d919748879884a518569e1ee7e5c97dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation duration, what's the situation: 0 days 00:00:53.101327\n",
      "\n",
      "-----TRAIN---------------\n",
      "     0.099 - final_train_loss\n",
      "     0.952 - final_train_accuracy\n",
      "     0.946 - final_train_f1\n",
      "     0.964 - final_train_precision\n",
      "     0.929 - final_train_recall\n",
      "    26.587 - final_train_runtime\n",
      "   561.140 - final_train_samples_per_second\n",
      "    17.565 - final_train_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "------TEST---------------\n",
      "     1.010 - final_test_loss\n",
      "     0.733 - final_test_accuracy\n",
      "     0.705 - final_test_f1\n",
      "     0.704 - final_test_precision\n",
      "     0.706 - final_test_recall\n",
      "    13.195 - final_test_runtime\n",
      "   565.309 - final_test_samples_per_second\n",
      "    17.735 - final_test_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n",
      "\n",
      "-----VALID---------------\n",
      "     1.006 - validation_loss\n",
      "     0.733 - validation_accuracy\n",
      "     0.707 - validation_f1\n",
      "     0.702 - validation_precision\n",
      "     0.713 - validation_recall\n",
      "    13.283 - validation_runtime\n",
      "   561.630 - validation_samples_per_second\n",
      "    17.617 - validation_steps_per_second\n",
      "     5.000 - epoch\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "time_evaluation_start = pd.Timestamp.now()\n",
    "\n",
    "final_metrics = {}\n",
    "final_metrics['train'] = trainer.evaluate(eval_dataset=ds_tokenized['train'], metric_key_prefix='final_train')\n",
    "final_metrics['test']= trainer.evaluate(eval_dataset=ds_tokenized['test'], metric_key_prefix='final_test')\n",
    "final_metrics['valid'] = trainer.evaluate(eval_dataset=ds_tokenized['valid'], metric_key_prefix='validation')\n",
    "\n",
    "time_evaluation_stop = pd.Timestamp.now()\n",
    "time_evaluation = time_evaluation_stop - time_evaluation_start\n",
    "\n",
    "print(\"\\nEvaluation duration, what's the situation:\", str(time_evaluation))\n",
    "\n",
    "# print the metrics\n",
    "for split in final_metrics:\n",
    "    print(f\"\\n{split.upper():->10}{'-'*15}\")\n",
    "    for k, v in final_metrics[split].items():\n",
    "        print(f\"{v:>10.3f} - {k}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 - Informal Test\n",
    "\n",
    "Feeding two completely made-up lines to the fine-tuned model, mostly for fun but also as a small test of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "         Test Line:  \"Assistant to the regional manager of beets, Mose and mother on the farm\"\n",
      " Predicted Speaker:  dwight\n",
      "            Logits:  tensor([[-1.5296,  1.1267]], device='cuda:0')\n",
      "--------------------------------------------------\n",
      "         Test Line:  \"My name is Michael Scott, paper is my business\"\n",
      " Predicted Speaker:  not_dwight\n",
      "            Logits:  tensor([[ 3.2646, -3.3140]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_lines = [\n",
    "    \"Assistant to the regional manager of beets, Mose and mother on the farm\",\n",
    "    \"My name is Michael Scott, paper is my business\",\n",
    "]\n",
    "\n",
    "for line in test_lines:\n",
    "    print(\"-\"*50)\n",
    "    informal_test(tokenizer, model, line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 - Discussion / Conclusions (on this attempt)\n",
    "\n",
    "| Metrics (Train/Test/Valid)            | Accuracy              | F1 Score              | Precision             | Recall                | Fine-Tuning Time |\n",
    "|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|------------------|\n",
    "| (1) Basic Transformer (DistilBERT)    | 0.961 / 0.810 / 0.806 | 0.875 / 0.330 / 0.322 | 0.978 / 0.417 / 0.402 | 0.791 / 0.274 / 0.268 | 0d 0h 9m 48s     |\n",
    "| (2) Mod: Different PLM (RoBERTa)      | 0.917 / 0.824 / 0.825 | 0.703 / 0.314 / 0.324 | 0.909 / 0.474 / 0.480 | 0.573 / 0.235 / 0.245 | 0d 0h 45m 23s    |\n",
    "| (3) Mod: Re-Balance Data (DistilBERT) | 0.952 / 0.733 / 0.733 | 0.946 / 0.705 / 0.707 | 0.964 / 0.704 / 0.702 | 0.929 / 0.706 / 0.713 | 0d 0h 7m 27s     |\n",
    "\n",
    "### 3.6.1 - Accuracy\n",
    "With the re-balanced class representation in our dataset, the trivial classifier accuracy is now about 54.7% ( $\\frac{16,334}{29,838}$ ). With that in mind, the validation accuracy for this attempt (73.3%) is a major improvement from prior attempts.\n",
    "\n",
    "### 3.6.2 - F1 Score\n",
    "As with accuracy, the F1 scores for train/test/validation are all improved. The test/validation F1 are still lower than training F1, but by a smaller margin / smaller proportion than observed in prior attempts.\n",
    "\n",
    "### 3.6.3 - Precision / Recall\n",
    "Both measures are approximately even (low 70's %) and, keeping with the trend, much improved over prior attempts.\n",
    "\n",
    "### 3.6.4 - Overall \"Modified Approach: Different Pretrained Model (RoBERTa)\" Conclusion\n",
    "While we are surely not at \"production-ready\" model performance, we can see encouraging improvements in our performance metrics. One caveat, however, is we oversampled *prior to* the train/test/validation split, so the validation data is artificially modified.\n",
    "\n",
    "> TODO: fix that ^^^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
